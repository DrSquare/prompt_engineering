{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51SqowskesWX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lWVmTynewg8"
      },
      "source": [
        "# Guidelines for Tuning Prompting Key Parameters\n",
        "In this lesson, you'll practice how the adjustment of key paramters affect large language model responses. This will help you to write write effective prompts for large language models.\n",
        "\n",
        "## Setup\n",
        "#### Load the API key and relevant Python libaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzpGVgLMfVke"
      },
      "source": [
        "For demonstation, I will use OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TvBkGmCxfccW"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9J_gcD1TW82p",
        "outputId": "ef01bc6f-39dd-4c90-e412-cd7643032358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key Loaded Successfully!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom dotenv import load_dotenv, find_dotenv\\n_ = load_dotenv(find_dotenv())\\n\\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\\n\\nfrom openai import OpenAI\\nclient = OpenAI()\\n\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Get secret API key\n",
        "\"\"\"\n",
        "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OpenAI API Key not found in Colab secrets!\")\n",
        "\"\"\"\n",
        "# Set it as environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'YOUR KEY'\n",
        "\n",
        "print(\"API Key Loaded Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ARh1wuhwI1"
      },
      "source": [
        "**helper function** <br>\n",
        "<br>\n",
        "Throughout this demo, we will use OpenAI's gpt-4o-mini model and the chat completions endpoint.\n",
        "<br>\n",
        "This helper function will make it easier to use prompts and look at the generated outputs. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q2k2dEg_p76W"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0.5, \\\n",
        "                   max_tokens=40, top_p=0.9, logprobs=False):\n",
        "     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "     response = client.chat.completions.create(\n",
        "         model=model,\n",
        "         messages=messages,\n",
        "         temperature=temperature,\n",
        "         max_tokens=max_tokens,\n",
        "         top_p=top_p,\n",
        "         logprobs=logprobs\n",
        "\n",
        "     )\n",
        "\n",
        "     return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX24OS0wr0zu"
      },
      "source": [
        "### 1. Max_Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-e9_m33HjRDs"
      },
      "outputs": [],
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"\n",
        "Write a short story about a space explorer who discovers a new planet.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbfSuaD0rv6s",
        "outputId": "2f9aa47f-0fc0-4a8d-9127-007df4699a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Captain Elara Voss\n"
          ]
        }
      ],
      "source": [
        "# Obtain responses: max_tokens = 5\n",
        "response = get_completion(prompt, max_tokens=5)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ-i1J3PipLf",
        "outputId": "7fcfe1d1-edba-4653-a86f-0481c0cadc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Captain Elara Voss had always been drawn to the stars. As the commander of the starship\n"
          ]
        }
      ],
      "source": [
        "# Obtain responses: max_tokens = 40\n",
        "response = get_completion(prompt, max_tokens=20)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLAjvAvNu3xl"
      },
      "source": [
        "###2. Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3Lv4mKI7pQJn"
      },
      "outputs": [],
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"\n",
        "Give me a list of 3 unique and unusual ice cream flavors.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gageJyjrvFWV",
        "outputId": "a04905c0-3f97-432b-f45d-5034c459ae1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here are three unique and unusual ice cream flavors:\n",
            "\n",
            "1. **Lavender Honey** - A delicate blend of floral lavender and sweet honey, this ice cream offers a soothing and aromatic experience, perfect for those who enjoy herbal and floral notes.\n",
            "\n",
            "2. **Balsamic Strawberry** - This flavor combines ripe strawberries with a drizzle of aged balsamic vinegar, creating a sweet and tangy treat that enhances the natural fruitiness with a sophisticated twist.\n",
            "\n",
            "3. **Wasabi Ginger** - A\n"
          ]
        }
      ],
      "source": [
        "# Obtain responses: temperature = 0\n",
        "response = get_completion(prompt, temperature=0, max_tokens=100)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaTHIFPgvSbL",
        "outputId": "f66ebb91-212c-4af9-c240-590c8c7d0512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here are three unique and unusual ice cream flavors:\n",
            "\n",
            "1. **Basil and Lime Sorbet** - A refreshing sorbet that combines the herbal notes of fresh basil with the zesty brightness of lime, creating a light and invigorating dessert.\n",
            "\n",
            "2. **Black Sesame and Coconut** - This flavor features the nutty richness of black sesame paired with the creamy sweetness of coconut, resulting in a complex and earthy ice cream that's both intriguing and delicious.\n",
            "\n",
            "3. **Avocado and Sea Salt\n"
          ]
        }
      ],
      "source": [
        "response = get_completion(prompt, temperature=2, max_tokens=100)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5GqXZBsvm-K"
      },
      "source": [
        "### 3. Top_P (nucleus sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qPDNp_UOw2F6"
      },
      "outputs": [],
      "source": [
        "# Define prompt\n",
        "prompt = \"\"\"\n",
        "Describe a sunset over a tropical beach.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miVwiGjxw807",
        "outputId": "f2ceac59-a112-462d-9caf-462cb37dcacf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As the sun begins its descent toward the horizon, the sky transforms into a breathtaking canvas of colors. Hues of deep orange and vibrant pink blend seamlessly with soft purples and gentle blues, creating a stunning gradient that reflects the beauty of the tropical setting. The sun, a fiery orb, casts a warm golden light over the tranquil waters, making them shimmer like liquid gold.\n",
            "\n",
            "The gentle waves lap rhythmically against the shore, their white foam contrasting with the rich, golden sands. Palm trees sway lightly\n"
          ]
        }
      ],
      "source": [
        "# Obtain responses: top_p = 0.1 (conservative and repetitive)\n",
        "response = get_completion(prompt, temperature=0.3, max_tokens=100, top_p=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mdH7Lp9xdX3",
        "outputId": "c7a0f9b0-4aac-48b0-dc13-9a8c04f62e00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As the sun begins its descent toward the horizon, the sky transforms into a breathtaking canvas of colors. Hues of deep orange and vibrant pink blend seamlessly with soft purples and gentle blues, casting a warm glow over the tranquil waters of the tropical beach. The sun, a fiery orb, hovers just above the horizon, its light shimmering on the surface of the waves, creating a pathway of gold that stretches toward the shore.\n",
            "\n",
            "Palm trees sway gently in the warm evening breeze, their silhouettes dark against\n"
          ]
        }
      ],
      "source": [
        "# Obtain responses: top_p = 0.9 (diverse output)\n",
        "response = get_completion(prompt, temperature=0.3, max_tokens=100, top_p=0.9)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my-l0KH9x2eT"
      },
      "source": [
        "### 4. log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPgLuZe3x-U-",
        "outputId": "e1e1f3bc-d450-494f-d535-b6db1a18979d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 1 Logprobs:\n",
            "Token: ..., Top Log Probabilities: [TopLogprob(token='The', bytes=[84, 104, 101], logprob=-0.7013113), TopLogprob(token='\"The', bytes=[34, 84, 104, 101], logprob=-0.8263113)]\n",
            "Token: j, Top Log Probabilities: [TopLogprob(token='j', bytes=[106], logprob=-0.00012892624), TopLogprob(token=' jumps', bytes=[32, 106, 117, 109, 112, 115], logprob=-9.250129)]\n",
            "Token: umps, Top Log Probabilities: [TopLogprob(token='umps', bytes=[117, 109, 112, 115], logprob=-1.9361265e-07), TopLogprob(token=' jumps', bytes=[32, 106, 117, 109, 112, 115], logprob=-16.625)]\n",
            "Token:  over, Top Log Probabilities: [TopLogprob(token=' over', bytes=[32, 111, 118, 101, 114], logprob=0.0), TopLogprob(token=' Over', bytes=[32, 79, 118, 101, 114], logprob=-17.0)]\n",
            "Token:  the, Top Log Probabilities: [TopLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=0.0), TopLogprob(token=' a', bytes=[32, 97], logprob=-18.0)]\n",
            "Token:  lazy, Top Log Probabilities: [TopLogprob(token=' lazy', bytes=[32, 108, 97, 122, 121], logprob=-0.00013250235), TopLogprob(token='lazy', bytes=[108, 97, 122, 121], logprob=-10.125133)]\n",
            "Token:  dog, Top Log Probabilities: [TopLogprob(token=' dog', bytes=[32, 100, 111, 103], logprob=-3.0545007e-06), TopLogprob(token=' Dog', bytes=[32, 68, 111, 103], logprob=-13.875003)]\n",
            "Token: ., Top Log Probabilities: [TopLogprob(token='.', bytes=[46], logprob=-0.0041279113), TopLogprob(token='.\\n\\n', bytes=[46, 10, 10], logprob=-5.504128)]\n",
            "Token:  This, Top Log Probabilities: [TopLogprob(token=' This', bytes=[32, 84, 104, 105, 115], logprob=-0.0011845153), TopLogprob(token=' \\n\\n', bytes=[32, 10, 10], logprob=-6.7511845)]\n",
            "Token:  sentence, Top Log Probabilities: [TopLogprob(token=' sentence', bytes=[32, 115, 101, 110, 116, 101, 110, 99, 101], logprob=-0.28851336), TopLogprob(token=' phrase', bytes=[32, 112, 104, 114, 97, 115, 101], logprob=-1.5385134)]\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"The quick brown fox\"\n",
        "\n",
        "response1 = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt1}],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    max_tokens=10,\n",
        "    logprobs=True,\n",
        "    top_logprobs = 2 # Request top 2 log probabilities for each token\n",
        ")\n",
        "\n",
        "# Print logprobs for both prompts\n",
        "print(\"Response 1 Logprobs:\")\n",
        "\n",
        "# Access the logprobs object\n",
        "logprobs = response1.choices[0].logprobs\n",
        "\n",
        "# Iterate through the content list within logprobs\n",
        "for logprobs_obj in logprobs.content:\n",
        "    # Now access top_logprobs for each item in content\n",
        "    if logprobs_obj.top_logprobs is not None:  # Check if top_logprobs exists\n",
        "        # Access the token text using logprobs_obj.token\n",
        "        print(f\"Token: {logprobs_obj.token}, Top Log Probabilities: {logprobs_obj.top_logprobs}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
